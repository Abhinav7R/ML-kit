"""
This is the KNN model. It is implemented from scratch without using any libraries.
"""

import numpy as np
import sys

# Note the code in #-------------------# is generated by chat gpt
# It is used to show the progress of the predict function

class initialKNN:
    def __init__(self, k=3, distance_metric='euclidean'):
        self.k = k
        self.distance_metric = distance_metric

    def fit(self, X, y):
        self.X_train = X
        self.unique_labels = np.unique(y)
        self.label_to_int = {label: idx for idx, label in enumerate(self.unique_labels)}
        self.int_to_label = {idx: label for label, idx in self.label_to_int.items()}
        self.y_train = np.vectorize(self.label_to_int.get)(y)

    def predict(self, X):

        #-------------------#
        sys.stdout.write("Progress: [")
        sys.stdout.flush()
        #-------------------#
        
        num_samples = len(X)
        y_pred_int = np.empty(num_samples)
        for i, x in enumerate(X):
            y_pred_int[i] = self._predict_individual(x)
            #-------------------#
            progress = (i + 1) / num_samples
            bar_length = 40
            block = int(round(bar_length * progress))
            progress_str = "#" * block + "-" * (bar_length - block)
            sys.stdout.write(f"\rProgress: [{progress_str}] {int(progress * 100)}%")
            sys.stdout.flush()
            #-------------------#

        #-------------------#
        sys.stdout.write("\n")
        sys.stdout.flush()
        #-------------------#

        y_pred = np.vectorize(self.int_to_label.get)(y_pred_int)
        return np.array(y_pred)

    def _predict_individual(self, x):
        distances = [self._calculate_distance(x, x_train) for x_train in self.X_train]
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]
        most_common = np.bincount(k_nearest_labels).argmax()
        return most_common

    def _calculate_distance(self, x1, x2):
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((x1 - x2)**2))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(x1 - x2))
        elif self.distance_metric == 'cosine':
            cosine_similarity = np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))
            return 1 - cosine_similarity
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")


class OptimisedKNN:
    def __init__(self, k=3, distance_metric='euclidean'):
        self.k = k
        self.distance_metric = distance_metric

    def fit(self, X, y):
        self.X_train = X
        self.unique_labels = np.unique(y)
        self.label_to_int = {label: idx for idx, label in enumerate(self.unique_labels)}
        self.int_to_label = {idx: label for label, idx in self.label_to_int.items()}
        self.y_train = np.vectorize(self.label_to_int.get)(y)

    def predict(self, X):
        num_batches = 500
        batches = np.array_split(X, num_batches)
        y_pred_int_list = []
        #-------------------#
        sys.stdout.write("Progress: [")
        sys.stdout.flush()
        #-------------------#

        for i, batch in enumerate(batches):
            batch_distances = self._calculate_distances(batch, self.X_train)
            y_pred_int_batch = self._assign_class_from_neighbour_distances(batch_distances)
            y_pred_int_list.append(y_pred_int_batch)
            #-------------------#
            progress = (i + 1) / num_batches
            bar_length = 40
            block = int(round(bar_length * progress))
            progress_str = "#" * block + "-" * (bar_length - block)
            sys.stdout.write(f"\rProgress: [{progress_str}] {int(progress * 100)}%")
            sys.stdout.flush()
            #-------------------#

        #-------------------#
        sys.stdout.write("\n")
        sys.stdout.flush()
        #-------------------#

        y_pred_int = np.concatenate(y_pred_int_list)
        return np.vectorize(self.int_to_label.get)(y_pred_int)

    def _calculate_distances(self, X, X_train):
        # basically for every data point in test set, calculate the distance to every data point in train set 
        # this is done at once using numpy vectorization for speed 
        # it returns a 2D array of shape (n_test, n_train) where each element (i, j) is the distance between the ith test point and the jth train point
        if self.distance_metric == 'euclidean':
            return np.sqrt(np.sum((X[:, None] - X_train[None]) ** 2, axis=-1))
        elif self.distance_metric == 'manhattan':
            return np.sum(np.abs(X[:, None] - X_train[None]), axis=-1)
        elif self.distance_metric == 'cosine':
            X_norm = np.linalg.norm(X, axis=1)[:, None]
            X_train_norm = np.linalg.norm(X_train, axis=1)[None]
            cosine_similarity = np.dot(X, X_train.T) / np.dot(X_norm, X_train_norm)
            return 1 - cosine_similarity
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

    def _assign_class_from_neighbour_distances(self, distances):
        k_indices = np.argsort(distances, axis=1)[:, :self.k]
        k_nearest_labels = self.y_train[k_indices]
        most_common = np.array([np.bincount(labels).argmax() for labels in k_nearest_labels])
        return most_common







        
#unit test

# knn1 = initialKNN(k=3, distance_metric='euclidean')

# X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
# y_train = np.array(["lol", "lol", "sad", "sad"])

# knn1.fit(X_train, y_train)

# X_test = np.array([[5, 6], [3, 4], [5, 6], [7, 8]])

# y_pred = knn1.predict(X_test)

# print(y_pred)